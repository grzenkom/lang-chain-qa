{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c515de6b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![Roche logo](images/roche-logo-blue-aligned-right.png)\n",
    "\n",
    "<p>&nbsp;</p>\n",
    "<p>&nbsp;</p>\n",
    "\n",
    "# Building a Q&A engine with LangChain and open-source LLMs\n",
    "## Marek Grzenkowicz\n",
    "\n",
    "#### September 2023"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a698d5f5",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## About Roche\n",
    "\n",
    "![Key Roche Informatics hubs](images/roche-about.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea2265f8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Large Language Models did not appear out of nowhere\n",
    "\n",
    "### Natural language processing (NLP)\n",
    "\n",
    "- Standardized tasks (question answering, summarization, sentiment analysis, ...)\n",
    "- Evaluation benchmarks\n",
    "- **Leaderboards**\n",
    "- Word and sentence **embeddings** (word2vec, GloVe, fastText, ELMo, BERT, ...)\n",
    "\n",
    "### Machine learning\n",
    "\n",
    "- Neural networks\n",
    "- Deep learning\n",
    "- CUDA\n",
    "- Transformer architecture\n",
    "- Attention (ML technique)\n",
    "- Reinforced learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3068c119",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Objectives\n",
    "\n",
    "1. Build an LLM application **without extensive NLP expertise**\n",
    "2. Enable **local development and execution** (easy to replace with cloud-hosted inference APIs later)\n",
    "3. Utilize **open-source models**\n",
    "4. Prototype a solution for a **real business challenge**\n",
    "    1. Query proprietary company documents using natural language (semantic search)\n",
    "    2. Create a ChatGPT-like user experience"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0def2e66",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Tools\n",
    "\n",
    "- [LangChain](https://python.langchain.com/docs/get_started/introduction.html) - framework for developing applications powered by LLMs\n",
    "- [Hugging Face Hub](https://huggingface.co/models) - repository of pre-trained language models\n",
    "  - [Transformers](https://huggingface.co/docs/transformers/index) - downloading the models\n",
    "- [lmsys/fastchat-t5-3b-v1.0](https://huggingface.co/lmsys/fastchat-t5-3b-v1.0) - compact, open-source LLM from [lmsys.org](https://lmsys.org) \n",
    "- [ChromaDB](https://www.trychroma.com) - embedding database (vector store)\n",
    "- [Jupyter Notebook](https://jupyter.org/) with the [RISE](https://github.com/damianavila/RISE) extension - IDE with a slideshow feature"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7f7334c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## It's full of LLMs!\n",
    "\n",
    "![LLM Evolutionary Tree](https://raw.githubusercontent.com/Mooler0410/LLMsPracticalGuide/main/imgs/tree.jpg)\n",
    "\n",
    "Source: [github.com/Mooler0410/LLMsPracticalGuide/](https://github.com/Mooler0410/LLMsPracticalGuide/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8afe1963",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Why the `lmsys/fastchat-t5-3b-v1.0` model?\n",
    "\n",
    "- GPU + 4GB memory - 1B parameters at best\n",
    "- CPU + 32GB RAM - 3B parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0c7e20a",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- [Open LLM leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard) âž¡ [`CobraMamba/mamba-gpt-3b-v3`](https://huggingface.co/CobraMamba/mamba-gpt-3b-v3) claims to surpass some 12B models\n",
    "  - but it is slow âž¡ [Open LLM performace leaderboard](https://huggingface.co/spaces/optimum/llm-perf-leaderboard)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d957d6e",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "- [lmsys/fastchat-t5-3b-v1.0](https://huggingface.co/lmsys/fastchat-t5-3b-v1.0)\n",
    "  - _A commercial-friendly, compact, yet powerful chat assistant_\n",
    "  - The first model to actually generate any response on my laptop in reasonable time ðŸŽ‰"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ee1ac86",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Some duct tape first ðŸ™ˆ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d2e4dd33",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-11T08:19:25.618758Z",
     "start_time": "2023-09-11T08:19:25.276739Z"
    },
    "execution": {
     "iopub.execute_input": "2023-08-17T21:41:02.423021Z",
     "iopub.status.busy": "2023-08-17T21:41:02.421305Z",
     "iopub.status.idle": "2023-08-17T21:41:02.437203Z",
     "shell.execute_reply": "2023-08-17T21:41:02.431892Z",
     "shell.execute_reply.started": "2023-08-17T21:41:02.422912Z"
    },
    "slideshow": {
     "slide_type": "-"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# careful! important warnings may get hidden\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import logging\n",
    "from transformers.utils import logging\n",
    "logging.set_verbosity(logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bbd4edbe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-11T08:19:25.630639Z",
     "start_time": "2023-09-11T08:19:25.620957Z"
    },
    "execution": {
     "iopub.execute_input": "2023-08-16T22:06:50.604289Z",
     "iopub.status.busy": "2023-08-16T22:06:50.601671Z",
     "iopub.status.idle": "2023-08-16T22:06:50.619156Z",
     "shell.execute_reply": "2023-08-16T22:06:50.616908Z",
     "shell.execute_reply.started": "2023-08-16T22:06:50.604128Z"
    },
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# https://github.com/chroma-core/chroma/blob/main/chromadb/__init__.py#L57\n",
    "\n",
    "import sys\n",
    "__import__(\"pysqlite3\")\n",
    "sys.modules[\"sqlite3\"] = sys.modules.pop(\"pysqlite3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "517f00e4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Load a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5e02bdd6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-12T11:42:30.469992Z",
     "start_time": "2023-09-12T11:42:30.465834Z"
    },
    "execution": {
     "iopub.execute_input": "2023-08-16T22:06:50.623104Z",
     "iopub.status.busy": "2023-08-16T22:06:50.621944Z",
     "iopub.status.idle": "2023-08-16T22:06:50.640917Z",
     "shell.execute_reply": "2023-08-16T22:06:50.637588Z",
     "shell.execute_reply.started": "2023-08-16T22:06:50.623017Z"
    },
    "slideshow": {
     "slide_type": "-"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.llms import HuggingFacePipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "035dd36a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-12T11:45:54.943071Z",
     "start_time": "2023-09-12T11:45:54.281244Z"
    },
    "execution": {
     "iopub.execute_input": "2023-08-16T22:06:50.645364Z",
     "iopub.status.busy": "2023-08-16T22:06:50.644624Z",
     "iopub.status.idle": "2023-08-16T22:07:33.764232Z",
     "shell.execute_reply": "2023-08-16T22:07:33.714360Z",
     "shell.execute_reply.started": "2023-08-16T22:06:50.645309Z"
    },
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Could not load the text-generation model due to missing dependencies.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[0;32m~/.pyenv/versions/3.10.13/envs/lang-chain-qa-env/lib/python3.10/site-packages/langchain/llms/huggingface_pipeline.py:91\u001b[0m, in \u001b[0;36mHuggingFacePipeline.from_model_id\u001b[0;34m(cls, model_id, task, device, model_kwargs, pipeline_kwargs, **kwargs)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m task \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext-generation\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 91\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m_model_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m task \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext2text-generation\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msummarization\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.13/envs/lang-chain-qa-env/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:563\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    562\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m _get_model_class(config, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\n\u001b[0;32m--> 563\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    564\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    565\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    566\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    567\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    568\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    569\u001b[0m )\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.13/envs/lang-chain-qa-env/lib/python3.10/site-packages/transformers/modeling_utils.py:2482\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   2481\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (is_accelerate_available() \u001b[38;5;129;01mand\u001b[39;00m is_bitsandbytes_available()):\n\u001b[0;32m-> 2482\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[1;32m   2483\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing `load_in_8bit=True` requires Accelerate: `pip install accelerate` and the latest version of\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2484\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m bitsandbytes `pip install -i https://test.pypi.org/simple/ bitsandbytes` or\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2485\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m pip install bitsandbytes` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2486\u001b[0m     )\n\u001b[1;32m   2488\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m torch_dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2489\u001b[0m     \u001b[38;5;66;03m# We force the `dtype` to be float16, this is a requirement from `bitsandbytes`\u001b[39;00m\n",
      "\u001b[0;31mImportError\u001b[0m: Using `load_in_8bit=True` requires Accelerate: `pip install accelerate` and the latest version of bitsandbytes `pip install -i https://test.pypi.org/simple/ bitsandbytes` or pip install bitsandbytes` ",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 18\u001b[0m\n\u001b[1;32m     13\u001b[0m model_id, task \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVoicelab/trurl-2-7b-8bit\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext-generation\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# model_id, task = \"Voicelab/trurl-2-13b\", \"text-generation\" - way too big!\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# model will be downloaded on first use and cached in ~/.cache/huggingface/hub/\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mHuggingFacePipeline\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_model_id\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43mload_in_8bit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtemperature\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_length\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mload_in_8bit\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[1;32m     26\u001b[0m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# -1 CPU\u001b[39;49;00m\n\u001b[1;32m     28\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.13/envs/lang-chain-qa-env/lib/python3.10/site-packages/langchain/llms/huggingface_pipeline.py:100\u001b[0m, in \u001b[0;36mHuggingFacePipeline.from_model_id\u001b[0;34m(cls, model_id, task, device, model_kwargs, pipeline_kwargs, **kwargs)\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m     96\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGot invalid task \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtask\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     97\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcurrently only \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mVALID_TASKS\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m are supported\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     98\u001b[0m         )\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 100\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    101\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not load the \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtask\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m model due to missing dependencies.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    102\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m importlib\u001b[38;5;241m.\u001b[39mutil\u001b[38;5;241m.\u001b[39mfind_spec(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    105\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: Could not load the text-generation model due to missing dependencies."
     ]
    }
   ],
   "source": [
    "# model_id, task = \"radlab/polish-gpt2-small\", \"text-generation\" - nonsensowne odpowiedzi\n",
    "# model_id, task = \"s3nh/DialoGPT-large-instruct-polish-3000-steps\", \"text-generation\" - nonsensowne odpowiedzi\n",
    "# model_id, task = \"s3nh/pythia-1.4b-deduped-53k-steps-self-instruct-polish\", \"text-generation\" - ???\n",
    "\n",
    "\n",
    "# https://huggingface.co/szymonrucinski/krakowiak-7b\n",
    "# https://huggingface.co/Voicelab/trurl-2-7b\n",
    "# https://huggingface.co/Voicelab/trurl-2-7b-8bit\n",
    "# https://huggingface.co/Voicelab/trurl-2-13b\n",
    "\n",
    "# model_id, task = \"szymonrucinski/krakowiak-7b\", \"text-generation\" - no config.json?\n",
    "# model_id, task = \"Voicelab/trurl-2-7b\", \"text-generation\" - OOM\n",
    "model_id, task = \"Voicelab/trurl-2-7b-8bit\", \"text-generation\"\n",
    "# model_id, task = \"Voicelab/trurl-2-13b\", \"text-generation\" - way too big!\n",
    "\n",
    "# model will be downloaded on first use and cached in ~/.cache/huggingface/hub/\n",
    "\n",
    "model = HuggingFacePipeline.from_model_id(\n",
    "    model_id=model_id,\n",
    "    task=task,\n",
    "    load_in_8bit=True,\n",
    "    model_kwargs={\n",
    "        \"temperature\": 0.1,\n",
    "        \"max_length\": 1000,\n",
    "        \"load_in_8bit\": True\n",
    "    },\n",
    "    device=0,  # -1 CPU\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83aecfba",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Initialize an LLM chain and start asking questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "356e652b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-11T13:13:08.729970Z",
     "start_time": "2023-09-11T13:13:08.726353Z"
    },
    "execution": {
     "iopub.execute_input": "2023-08-16T22:07:33.872300Z",
     "iopub.status.busy": "2023-08-16T22:07:33.858476Z",
     "iopub.status.idle": "2023-08-16T22:07:34.086456Z",
     "shell.execute_reply": "2023-08-16T22:07:34.082581Z",
     "shell.execute_reply.started": "2023-08-16T22:07:33.864984Z"
    },
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate, LLMChain\n",
    "\n",
    "template_text = \"\"\"\n",
    "{question}\n",
    "\"\"\"\n",
    "template = PromptTemplate(template=template_text, input_variables=[\"question\"])\n",
    "\n",
    "llm_chain = LLMChain(llm=model, prompt=template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ca4756f2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-11T08:21:23.404953Z",
     "start_time": "2023-09-11T08:20:05.275652Z"
    },
    "execution": {
     "iopub.execute_input": "2023-08-16T22:07:34.093984Z",
     "iopub.status.busy": "2023-08-16T22:07:34.093054Z",
     "iopub.status.idle": "2023-08-16T22:08:23.115323Z",
     "shell.execute_reply": "2023-08-16T22:08:23.114319Z",
     "shell.execute_reply.started": "2023-08-16T22:07:34.093904Z"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<pad> Sheryl  Crow  is  an  American  singer,  songwriter,  and  actress.  She  is  best  known  for  her  role  as  the  lead  singer  and  lead  guitarist  of  the  rock  band  The  Band wagon,  and  for  her  role  as  the  lead  singer  and  lead  guitarist  of  the  alternative  rock  band  The  Mamas  and  the  Papas.  Crow  has  also  been  a  member  of  the  band  The  Mamas  and  the  Papas  since  its  formation  in  1995.\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_chain(\"Who is Sheryl Crow?\")[\"text\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45e5b74f",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## 2019: Language models cannot augment knowledge automatically\n",
    "\n",
    "!['Run, Baby, Run' and deaths of Huxley and Kennedy](./images/sheryl-crow-huxley-kennedy.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a9190d2",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## 2023: ChatGPT still fails at that! ðŸ˜® (for this particular example)\n",
    "\n",
    "!['Run, Baby, Run' confuses ChatGPT](./images/sheryl-crow-chatgpt.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26aafabb",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Prompt engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3863d1ce",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-11T14:10:48.297199Z",
     "start_time": "2023-09-11T14:10:48.198267Z"
    },
    "execution": {
     "iopub.execute_input": "2023-08-16T22:08:23.120133Z",
     "iopub.status.busy": "2023-08-16T22:08:23.119666Z",
     "iopub.status.idle": "2023-08-16T22:08:23.127261Z",
     "shell.execute_reply": "2023-08-16T22:08:23.125549Z",
     "shell.execute_reply.started": "2023-08-16T22:08:23.120104Z"
    },
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "template_text = \"\"\"Odpowiadaj krÃ³tkimi zdaniami.\n",
    "Pytanie: {question}\n",
    "\"\"\"\n",
    "template = PromptTemplate(template=template_text, input_variables=[\"question\"])\n",
    "\n",
    "llm_chain = LLMChain(llm=model, prompt=template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e1be847d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-11T14:12:15.092039Z",
     "start_time": "2023-09-11T14:11:31.954495Z"
    },
    "execution": {
     "iopub.execute_input": "2023-08-16T22:08:23.144759Z",
     "iopub.status.busy": "2023-08-16T22:08:23.139118Z",
     "iopub.status.idle": "2023-08-16T22:08:26.141136Z",
     "shell.execute_reply": "2023-08-16T22:08:26.139836Z",
     "shell.execute_reply.started": "2023-08-16T22:08:23.144710Z"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nA) czarny\\nB) zielony\\nC) niebieski\\nD) Å¼Ã³Å‚ty\\n\\n### WyjÅ›cie:\\nB) zielony\\n\\n### Zadanie:\\n\"Przepisz podane zdanie, aby wyraziÄ‡ jednÄ… moÅ¼liwÄ… opiniÄ™.\"\\n\\n### Input:\\n\"ZjadÅ‚em jabÅ‚ko wczoraj.\"\\n|\\n### Response:\\n\"ZjadÅ‚em jabÅ‚ko wczoraj.\"\\n'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_chain(\"Jakiego koloru jest trawa?\")[\"text\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2743b4e8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Easy questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d8c8ce7f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-11T08:21:28.260969Z",
     "start_time": "2023-09-11T08:21:26.338014Z"
    },
    "execution": {
     "iopub.execute_input": "2023-08-16T22:08:26.146251Z",
     "iopub.status.busy": "2023-08-16T22:08:26.144220Z",
     "iopub.status.idle": "2023-08-16T22:08:28.416205Z",
     "shell.execute_reply": "2023-08-16T22:08:28.414969Z",
     "shell.execute_reply.started": "2023-08-16T22:08:26.146142Z"
    },
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<pad> Europe'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_chain(\"Where is Poland located?\")[\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5f13f29e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-11T08:21:36.709710Z",
     "start_time": "2023-09-11T08:21:28.265412Z"
    },
    "execution": {
     "iopub.execute_input": "2023-08-16T22:08:28.441585Z",
     "iopub.status.busy": "2023-08-16T22:08:28.440090Z",
     "iopub.status.idle": "2023-08-16T22:08:35.085948Z",
     "shell.execute_reply": "2023-08-16T22:08:35.084869Z",
     "shell.execute_reply.started": "2023-08-16T22:08:28.441507Z"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<pad> Bialowieza Forest is a protected forest in Poland.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_chain(\"What is the Bialowieza Forest?\")[\"text\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca50ed8e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Harder questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5f28f639",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-11T08:21:41.732441Z",
     "start_time": "2023-09-11T08:21:36.712382Z"
    },
    "execution": {
     "iopub.execute_input": "2023-08-16T22:08:35.093745Z",
     "iopub.status.busy": "2023-08-16T22:08:35.092959Z",
     "iopub.status.idle": "2023-08-16T22:08:40.537052Z",
     "shell.execute_reply": "2023-08-16T22:08:40.535729Z",
     "shell.execute_reply.started": "2023-08-16T22:08:35.093666Z"
    },
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<pad> \"Birch Tree\"'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_chain(\"What does the name 'Bialowieza' mean in English?\")[\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6066e8c9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-11T08:21:56.781262Z",
     "start_time": "2023-09-11T08:21:41.735084Z"
    },
    "execution": {
     "iopub.execute_input": "2023-08-16T22:08:40.539606Z",
     "iopub.status.busy": "2023-08-16T22:08:40.538716Z",
     "iopub.status.idle": "2023-08-16T22:08:50.661029Z",
     "shell.execute_reply": "2023-08-16T22:08:50.659823Z",
     "shell.execute_reply.started": "2023-08-16T22:08:40.539565Z"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<pad> The Tsar's Trail is a 900 mile long trail that begins in Moscow and ends in St. Petersburg.\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_chain(\"What's the length of the Tsar's Trail and where does it begin?\")[\"text\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5045560d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# What now?\n",
    "\n",
    "# Should I fine-tune the base model? ðŸ¤”"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15671b76",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Embeddings as a vector representation of text\n",
    "\n",
    "![cosine similarity in 2D](./images/vectors-cos-sim-500.png)\n",
    "\n",
    "The actual embedding spaces have **100s or even 1000s of dimensions**! ðŸ¤¯\n",
    "\n",
    "Source: [github.com/grzenkom/do-androids-read/](https://github.com/grzenkom/do-androids-read/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6efd3c3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Cosine similarity\n",
    "\n",
    "Cosine similarity is one of many vector similarity and vector distance measures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d49c5278",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-11T08:22:00.685783Z",
     "start_time": "2023-09-11T08:21:56.783731Z"
    },
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  1.233  ,   4.2963 ,  -7.9738 , -10.121  ,   1.8207 ,   1.4098 ,\n",
       "        -4.518  ,  -5.2261 ,  -0.29157,   0.95234], dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "vector_dog = nlp.vocab[u\"dog\"].vector\n",
    "vector_dog[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cec6a4fb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-11T08:22:00.697704Z",
     "start_time": "2023-09-11T08:22:00.687971Z"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cos(dog, husky) =  0.811\n",
      "cos(dog, cows ) =  0.352\n",
      "cos(dog, stone) =  0.071\n",
      "cos(dog, Xerox) = -0.134\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "for word in [\"husky\", \"cows\", \"stone\", \"Xerox\"]:\n",
    "    print(f\"cos(dog, {word:<5}) = {cosine_similarity([vector_dog], [nlp.vocab[word].vector])[0][0]:>6.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5321677",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## Arithmetic of word vectors\n",
    "\n",
    "\\begin{equation}\n",
    "\\LARGE{\\mathit{ v_{parent} + v_{woman} \\approx v_{x} }}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "565a48e4",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "![sum of \"parent\" and \"woman\" vectors](./images/vector-mother.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af7a39a6",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "\\begin{equation}\n",
    "\\LARGE{\\mathit{ v_{seawater} - v_{salt} \\approx v_{x} }}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c20367a",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "![difference of \"seawater\" and \"salt\" vectors](./images/vector-water.png)\n",
    "\n",
    "Source: [github.com/grzenkom/do-androids-read/](https://github.com/grzenkom/do-androids-read/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10d63fce",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Retrieval Augmented Generation (RAG)\n",
    "\n",
    "With RAG, documents can be stored as embeddings in a vector database, queried\n",
    "for based on semantic meaning, and then these relevant splits are passed into\n",
    "**model prompt via the context window**. LLM uses these text chunks from\n",
    "original documents to generate an answer.\n",
    "\n",
    "![Question Answering flow](images/langchain-qa-flow.jpg)\n",
    "\n",
    "Source: [python.langchain.com/docs/use_cases/question_answering/](https://python.langchain.com/docs/use_cases/question_answering/#overview)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f97e1686",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## LangChain integrations\n",
    "\n",
    "1. [Catalog](https://integrations.langchain.com/llms)\n",
    "2. [Documentation](https://python.langchain.com/docs/integrations)\n",
    "\n",
    "![LangChain integrations](./images/langchain-integrations.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ce7c79d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-29T18:39:47.648775Z",
     "start_time": "2023-08-29T18:39:47.644757Z"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## RAG step 1 - load documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1e971853",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-11T08:22:24.368287Z",
     "start_time": "2023-09-11T08:22:00.700254Z"
    },
    "execution": {
     "iopub.execute_input": "2023-08-16T22:08:50.663435Z",
     "iopub.status.busy": "2023-08-16T22:08:50.662895Z",
     "iopub.status.idle": "2023-08-16T22:09:09.419282Z",
     "shell.execute_reply": "2023-08-16T22:09:09.417508Z",
     "shell.execute_reply.started": "2023-08-16T22:08:50.663380Z"
    },
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from langchain.document_loaders import WikipediaLoader\n",
    "\n",
    "loader = WikipediaLoader(query=\"BiaÅ‚owieÅ¼a Forest\", lang=\"en\")\n",
    "bf_docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "040ddb4d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-11T08:22:25.249277Z",
     "start_time": "2023-09-11T08:22:24.373121Z"
    },
    "execution": {
     "iopub.execute_input": "2023-08-16T22:09:09.424667Z",
     "iopub.status.busy": "2023-08-16T22:09:09.422729Z",
     "iopub.status.idle": "2023-08-16T22:09:10.085782Z",
     "shell.execute_reply": "2023-08-16T22:09:10.084546Z",
     "shell.execute_reply.started": "2023-08-16T22:09:09.424571Z"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from langchain.document_loaders import WebBaseLoader\n",
    "\n",
    "loader = WebBaseLoader(\n",
    "    \"https://bpn.com.pl/index.php?option=com_content&task=view&id=651&Itemid=297&lang=en\"\n",
    ")\n",
    "bpn_page = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e47197f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-29T18:39:47.648775Z",
     "start_time": "2023-08-29T18:39:47.644757Z"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## RAG step 2 - split documents into chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ac9b0f97",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-11T08:22:25.265918Z",
     "start_time": "2023-09-11T08:22:25.251633Z"
    },
    "execution": {
     "iopub.execute_input": "2023-08-16T22:09:10.089999Z",
     "iopub.status.busy": "2023-08-16T22:09:10.087364Z",
     "iopub.status.idle": "2023-08-16T22:09:10.228097Z",
     "shell.execute_reply": "2023-08-16T22:09:10.225802Z",
     "shell.execute_reply.started": "2023-08-16T22:09:10.089875Z"
    },
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# RecursiveCharacterTextSplitter splits text recursively on characters\n",
    "# [\"\\n\\n\", \"\\n\", \" \", \"\"] and stops as soon as the chunks are small enough\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500, chunk_overlap=0\n",
    ")\n",
    "all_splits = text_splitter.split_documents(bf_docs + bpn_page)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7740590d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-29T18:39:47.648775Z",
     "start_time": "2023-08-29T18:39:47.644757Z"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## RAG step 3 - initialize `sentence_transformers` embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2e24add8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-11T08:22:27.457317Z",
     "start_time": "2023-09-11T08:22:25.268686Z"
    },
    "execution": {
     "iopub.execute_input": "2023-08-16T22:09:10.231405Z",
     "iopub.status.busy": "2023-08-16T22:09:10.230699Z",
     "iopub.status.idle": "2023-08-16T22:09:33.259577Z",
     "shell.execute_reply": "2023-08-16T22:09:33.258129Z",
     "shell.execute_reply.started": "2023-08-16T22:09:10.231350Z"
    },
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "hf_embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/all-mpnet-base-v2\",\n",
    "    model_kwargs={\n",
    "        \"device\": \"cpu\"\n",
    "    },\n",
    "    encode_kwargs={\n",
    "        \"normalize_embeddings\": False\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbeabe23",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-29T18:39:47.648775Z",
     "start_time": "2023-08-29T18:39:47.644757Z"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## RAG step 4 - calculate embeddings for text chunks and store them in a database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dc27f47d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-11T08:22:43.992045Z",
     "start_time": "2023-09-11T08:22:27.459424Z"
    },
    "execution": {
     "iopub.execute_input": "2023-08-16T22:09:10.231405Z",
     "iopub.status.busy": "2023-08-16T22:09:10.230699Z",
     "iopub.status.idle": "2023-08-16T22:09:33.259577Z",
     "shell.execute_reply": "2023-08-16T22:09:33.258129Z",
     "shell.execute_reply.started": "2023-08-16T22:09:10.231350Z"
    },
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "vector_store = Chroma.from_documents(\n",
    "    documents=all_splits, embedding=hf_embeddings\n",
    ")\n",
    "\n",
    "# default similarity measure is `l2`, not `cosine`\n",
    "#  - https://docs.trychroma.com/usage-guide#changing-the-distance-function\n",
    "#  - https://github.com/nmslib/hnswlib/tree/master#python-bindings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ab34f81",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-29T18:39:47.648775Z",
     "start_time": "2023-08-29T18:39:47.644757Z"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## RAG detour - test similarity search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "39b07025",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-11T08:46:36.974540Z",
     "start_time": "2023-09-11T08:46:36.910052Z"
    },
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " score  | size | document chunk\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['0.499 |  323 | BiaÅ‚owieÅ¼a [bÊ²awÉ”ËˆvÊ²É›Êa] (Belarusian: Ð‘ÐµÐ»Ð°Ð²ÐµÐ¶Ð°, BieÅ‚avieÅ¾a)  ...',\n",
       " '0.613 |  238 | BiaÅ‚owieÅ¼a is the seat of the administrative district of Gmi ...',\n",
       " '0.676 |  283 | == Location ==\\nBiaÅ‚owieÅ¼a is in eastern Poland, in Podlasie  ...',\n",
       " '0.716 |   95 | village of BiaÅ‚owieÅ¼a lies within the forest. BiaÅ‚owieÅ¼a mea ...',\n",
       " '0.748 |  498 | The BiaÅ‚owieÅ¼a Forest takes its name from the Polish village ...']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# similarity_search_with_score - lower score represents more similarity\n",
    "\n",
    "# examples: BiaÅ‚owieÅ¼a, bison, nature protection, pancake recipe\n",
    "relevant_splits = vector_store.similarity_search_with_score(\"BiaÅ‚owieÅ¼a\", k=5)\n",
    "print(\" score  | size | document chunk\")\n",
    "[\n",
    "    f'{score:.3f} | {len(chunk.page_content):>4} | {chunk.page_content[:60]} ...'\n",
    "    for chunk, score in relevant_splits\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89ee07d5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-29T18:39:47.648775Z",
     "start_time": "2023-08-29T18:39:47.644757Z"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## RAG step 5 - initialize a Q&A chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4df87ac3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-11T08:22:44.086289Z",
     "start_time": "2023-09-11T08:22:44.080073Z"
    },
    "execution": {
     "iopub.execute_input": "2023-08-16T22:09:33.262533Z",
     "iopub.status.busy": "2023-08-16T22:09:33.261903Z",
     "iopub.status.idle": "2023-08-16T22:09:34.035364Z",
     "shell.execute_reply": "2023-08-16T22:09:34.034420Z",
     "shell.execute_reply.started": "2023-08-16T22:09:33.262483Z"
    },
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "# the QA chain is constructed with the LLM model (loaded previously)\n",
    "# and the embedding database\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=model, retriever=vector_store.as_retriever()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da2a814",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### `RetrievalQA` chain - processing steps\n",
    "\n",
    "1. fetch document splits relevant to the question from the vector database,\n",
    "2. include the retrieved splits in the model prompt,\n",
    "3. have the LLM generate an answer based on original documents."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07b9f19b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-29T18:39:47.648775Z",
     "start_time": "2023-08-29T18:39:47.644757Z"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## RAG step 6 - generate answers with the Q&A chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "edbc4533",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-11T08:23:12.601605Z",
     "start_time": "2023-09-11T08:22:44.089636Z"
    },
    "execution": {
     "iopub.execute_input": "2023-08-16T22:09:34.038393Z",
     "iopub.status.busy": "2023-08-16T22:09:34.037898Z",
     "iopub.status.idle": "2023-08-16T22:10:09.259053Z",
     "shell.execute_reply": "2023-08-16T22:10:09.257612Z",
     "shell.execute_reply.started": "2023-08-16T22:09:34.038357Z"
    },
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<pad> The  name  'Bialowieza'  means  'White  Tower'  in  English.\\n\""
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"What does the name 'Bialowieza' mean in English?\"\n",
    "\n",
    "qa_chain(f\"Provide brief answers, use 10 words or less. {question}\")[\"result\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2e644748",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-11T08:23:35.961035Z",
     "start_time": "2023-09-11T08:23:12.604377Z"
    },
    "execution": {
     "iopub.execute_input": "2023-08-16T22:10:09.261416Z",
     "iopub.status.busy": "2023-08-16T22:10:09.260834Z",
     "iopub.status.idle": "2023-08-16T22:10:46.003982Z",
     "shell.execute_reply": "2023-08-16T22:10:45.998835Z",
     "shell.execute_reply.started": "2023-08-16T22:10:09.261366Z"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<pad> 4  km  long,  starts  at  Przed  Kosym  Mostem  depot.\\n'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"What's the length of the Tsar's Trail and where does it begin?\"\n",
    "\n",
    "qa_chain(f\"Provide brief answers, use 10 words or less. {question}\")[\"result\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "796e380a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## No more hallucinations then?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f7547af8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-11T08:23:49.594558Z",
     "start_time": "2023-09-11T08:23:35.965028Z"
    },
    "execution": {
     "iopub.execute_input": "2023-08-16T22:10:46.006264Z",
     "iopub.status.busy": "2023-08-16T22:10:46.005570Z",
     "iopub.status.idle": "2023-08-16T22:11:22.084775Z",
     "shell.execute_reply": "2023-08-16T22:11:22.082796Z",
     "shell.execute_reply.started": "2023-08-16T22:10:46.006221Z"
    },
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<pad> No,  only  scientists  can  navigate  freely.\\n'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"Is Bialowieza Forest famous for its walking trails?\"\n",
    "\n",
    "qa_chain(f\"Provide brief answers, use 10 words or less. {question}\")[\"result\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b9a066a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Next steps\n",
    "\n",
    "1. Other data loaders\n",
    "    - PDF files\n",
    "    - slide decks\n",
    "    - Google Sites\n",
    "    - files in Google Drive\n",
    "2. Conversational interface\n",
    "3. Memory of the conversation\n",
    "4. Larger model\n",
    "5. GPU processing\n",
    "6. Other chunking strategies\n",
    "    - find optimal chunk size\n",
    "    - ensure chunks preserve the context of the source documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25c569ce",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Links\n",
    "\n",
    "### Overview\n",
    "\n",
    "- [How OpenAI trained ChatGPT](https://blog.quastor.org/p/openai-trained-chatgpt)\n",
    "    - ðŸŽ¥ [State of GPT | Andrej Karpathy](https://www.youtube.com/watch?v=s6zNXZaIiiI)\n",
    "- [Catching up on the weird world of LLMs](https://simonwillison.net/2023/Aug/3/weird-world-of-llms)\n",
    "- [What We Know About LLMs  (Primer)](https://willthompson.name/what-we-know-about-llms-primer)\n",
    "- [The history, timeline, and future of LLMs](https://toloka.ai/blog/history-of-llms)\n",
    "- [The Practical Guides for Large Language Models](https://github.com/Mooler0410/LLMsPracticalGuide)\n",
    "- [The Many Ways that Digital Minds Can Know](https://moultano.wordpress.com/2023/06/28/the-many-ways-that-digital-minds-can-know)\n",
    "\n",
    "### Courses, tutorials\n",
    "\n",
    "- ðŸ‘©â€ðŸŽ“ [Large Language Models with Semantic Search](https://learn.deeplearning.ai/large-language-models-semantic-search) (great explanation of **embeddings** + sandbox to experiment with provided code samples)\n",
    "- [Running a Hugging Face Large Language Model (LLM) locally on my laptop](https://www.markhneedham.com/blog/2023/06/23/hugging-face-run-llm-model-locally-laptop)\n",
    "- [Why You (Probably) Donâ€™t Need to Fine-tune an LLM](https://www.tidepool.so/2023/08/17/why-you-probably-dont-need-to-fine-tune-an-llm/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7940b3df",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Links (cont.)\n",
    "\n",
    "### Challenges and future research\n",
    "\n",
    "- [Open challenges in LLM research](https://huyenchip.com/2023/08/16/llm-research-open-challenges.html) \n",
    "- [Getting from Generative AI to Trustworthy AI: What LLMs might learn from Cyc](https://arxiv.org/abs/2308.04445)\n",
    "- [OWASP Top 10 for Large Language Model Applications](https://owasp.org/www-project-top-10-for-large-language-model-applications) \n",
    "\n",
    "### Skepticism\n",
    "\n",
    "- ðŸŽ¥ [Why AI Is Incredibly Smart and Shockingly Stupid | Yejin Choi](https://www.youtube.com/watch?v=SvBR0OGT5VI)\n",
    "- [Anti-hype LLM reading list](https://gist.github.com/veekaybee/be375ab33085102f9027853128dc5f0e)\n",
    "- [What if Generative AI turned out to be a Dud?](https://garymarcus.substack.com/p/what-if-generative-ai-turned-out)\n",
    "    - And [Marcus on AI](https://garymarcus.substack.com) in general"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fbec57e",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "![Roche logo](images/roche-logo-blue-aligned-right.png)\n",
    "\n",
    "<p>&nbsp;</p>\n",
    "<p>&nbsp;</p>\n",
    "\n",
    "\n",
    "# Doing now what patients need next\n",
    "\n",
    "<p>&nbsp;</p>\n",
    "<p>&nbsp;</p>\n",
    "\n",
    "\n",
    "## ðŸ¦Š GitLab repository: [code.roche.com/grzenkom/lang-chain-qa](https://code.roche.com/grzenkom/lang-chain-qa)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8d74cf2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![Roche logo](images/roche-logo-blue-aligned-right.png)\n",
    "\n",
    "<p>&nbsp;</p>\n",
    "<p>&nbsp;</p>\n",
    "\n",
    "\n",
    "# Doing now what patients need next\n",
    "\n",
    "<p>&nbsp;</p>\n",
    "<p>&nbsp;</p>\n",
    "\n",
    "\n",
    "## â¬ GitHub repository: [go.roche.com/zsbit](https://go.roche.com/zsbit)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
